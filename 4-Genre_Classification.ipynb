{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: h5py in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, LSTM, Reshape\n",
    "from keras import regularizers\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=64, dim=(256,256, 1), n_channels=1,\n",
    "                 n_classes=5, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.reshape(np.load(ID), (256, 256, 1, 1))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 13985\n",
      "Validation Size:  6000\n"
     ]
    }
   ],
   "source": [
    "def add_Genre(partition, labels, genre, label):\n",
    "    path = 'mel-spectrograms/' + genre\n",
    "    files = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    partition['train'].extend(files[1500:len(files)])\n",
    "    partition['validation'].extend(files[0:1500])\n",
    "    \n",
    "    for file in files:\n",
    "        labels[file] = label\n",
    "        \n",
    "    return partition, labels\n",
    "\n",
    "partition = {'train':[], 'validation':[]}\n",
    "labels = {}\n",
    "\n",
    "partition, labels = add_Genre(partition, labels, 'Folk', 0)\n",
    "partition, labels = add_Genre(partition, labels, 'Hip-Hop', 1)\n",
    "partition, labels = add_Genre(partition, labels, 'Instrumental', 2)\n",
    "partition, labels = add_Genre(partition, labels, 'Pop', 3)\n",
    "#partition, labels = add_Genre(partition, labels, 'Experimental', 4)\n",
    "\n",
    "print('Training Size:', len(partition['train']))\n",
    "print('Validation Size: ', len(partition['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               204900    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 252,716\n",
      "Trainable params: 252,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((256, 256, 1), input_shape=(256, 256, 1, 1)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "218/218 [==============================] - 47s 217ms/step - loss: 2.0108 - acc: 0.4666 - val_loss: 1.5348 - val_acc: 0.5954\n",
      "Epoch 2/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 1.4103 - acc: 0.5806 - val_loss: 1.2035 - val_acc: 0.6438\n",
      "Epoch 3/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 1.1744 - acc: 0.6231 - val_loss: 1.0902 - val_acc: 0.6238\n",
      "Epoch 4/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 1.0617 - acc: 0.6375 - val_loss: 0.9535 - val_acc: 0.6747\n",
      "Epoch 5/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.9961 - acc: 0.6578 - val_loss: 0.9233 - val_acc: 0.6660\n",
      "Epoch 6/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.9501 - acc: 0.6656 - val_loss: 0.8865 - val_acc: 0.6729\n",
      "Epoch 7/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.9103 - acc: 0.6802 - val_loss: 0.8495 - val_acc: 0.6989\n",
      "Epoch 8/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.8790 - acc: 0.6904 - val_loss: 0.8378 - val_acc: 0.6979\n",
      "Epoch 9/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.8537 - acc: 0.7041 - val_loss: 0.8313 - val_acc: 0.7006\n",
      "Epoch 10/50\n",
      "218/218 [==============================] - 42s 194ms/step - loss: 0.8395 - acc: 0.7054 - val_loss: 0.8259 - val_acc: 0.7041\n",
      "Epoch 11/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.8295 - acc: 0.7101 - val_loss: 0.8309 - val_acc: 0.6919\n",
      "Epoch 12/50\n",
      "Epoch 11/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.8154 - acc: 0.7180 - val_loss: 0.8288 - val_acc: 0.7004\n",
      "Epoch 13/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.7904 - acc: 0.7248 - val_loss: 0.8442 - val_acc: 0.6882\n",
      "Epoch 14/50\n",
      "218/218 [==============================] - 42s 194ms/step - loss: 0.7957 - acc: 0.7246 - val_loss: 0.8408 - val_acc: 0.6988\n",
      "Epoch 15/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.7761 - acc: 0.7310 - val_loss: 0.8131 - val_acc: 0.7045\n",
      "Epoch 16/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.7817 - acc: 0.7284 - val_loss: 0.8165 - val_acc: 0.7083\n",
      "Epoch 17/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.7594 - acc: 0.7396 - val_loss: 0.8242 - val_acc: 0.6986\n",
      "Epoch 18/50\n",
      "218/218 [==============================] - 43s 195ms/step - loss: 0.7588 - acc: 0.7423 - val_loss: 0.7977 - val_acc: 0.7156\n",
      "Epoch 19/50\n",
      "\n",
      "218/218 [==============================] - 42s 194ms/step - loss: 0.7494 - acc: 0.7450 - val_loss: 0.8423 - val_acc: 0.7006\n",
      "Epoch 20/50\n",
      "218/218 [==============================] - 42s 193ms/step - loss: 0.7515 - acc: 0.7445 - val_loss: 0.7924 - val_acc: 0.7186\n",
      "Epoch 21/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.7349 - acc: 0.7504 - val_loss: 0.8270 - val_acc: 0.7046\n",
      "Epoch 22/50\n",
      "Epoch 21/50\n",
      "217/218 [============================>.] - ETA: 0s - loss: 0.7252 - acc: 0.7550Epoch 22/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.7254 - acc: 0.7549 - val_loss: 0.8147 - val_acc: 0.7100\n",
      "Epoch 23/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.7243 - acc: 0.7527 - val_loss: 0.7988 - val_acc: 0.7139\n",
      "Epoch 24/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.7241 - acc: 0.7554 - val_loss: 0.8262 - val_acc: 0.7109\n",
      "Epoch 25/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.7110 - acc: 0.7596 - val_loss: 0.8408 - val_acc: 0.7016\n",
      "Epoch 26/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.7081 - acc: 0.7610 - val_loss: 0.8337 - val_acc: 0.6991\n",
      "Epoch 27/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.7001 - acc: 0.7638 - val_loss: 0.8459 - val_acc: 0.6996\n",
      "Epoch 28/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.6994 - acc: 0.765 - 42s 191ms/step - loss: 0.6996 - acc: 0.7655 - val_loss: 0.8136 - val_acc: 0.7120\n",
      "Epoch 29/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6990 - acc: 0.7633 - val_loss: 0.8743 - val_acc: 0.6845\n",
      "Epoch 30/50\n",
      "Epoch 29/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6950 - acc: 0.7656 - val_loss: 0.7948 - val_acc: 0.7208\n",
      "Epoch 31/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6983 - acc: 0.7655 - val_loss: 0.8105 - val_acc: 0.7098\n",
      "Epoch 32/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6855 - acc: 0.7708 - val_loss: 0.8238 - val_acc: 0.7107\n",
      "Epoch 33/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6859 - acc: 0.7710 - val_loss: 0.8152 - val_acc: 0.7103\n",
      "Epoch 34/50\n",
      "218/218 [==============================] - 42s 192ms/step - loss: 0.6882 - acc: 0.7681 - val_loss: 0.8089 - val_acc: 0.7193\n",
      "Epoch 35/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6695 - acc: 0.7759 - val_loss: 0.8316 - val_acc: 0.7085\n",
      "Epoch 36/50\n",
      "Epoch 35/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6705 - acc: 0.7780 - val_loss: 0.8373 - val_acc: 0.6964\n",
      "Epoch 37/50\n",
      "Epoch 36/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6800 - acc: 0.7736 - val_loss: 0.8183 - val_acc: 0.7119\n",
      "Epoch 38/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6772 - acc: 0.7791 - val_loss: 0.8181 - val_acc: 0.7134\n",
      "Epoch 39/50\n",
      "218/218 [==============================] - 42s 190ms/step - loss: 0.6633 - acc: 0.7835 - val_loss: 0.8256 - val_acc: 0.7061\n",
      "Epoch 40/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6714 - acc: 0.7762 - val_loss: 0.8465 - val_acc: 0.7031\n",
      "Epoch 41/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6659 - acc: 0.7781 - val_loss: 0.8289 - val_acc: 0.7072\n",
      "Epoch 42/50\n",
      "\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6597 - acc: 0.7800 - val_loss: 0.8554 - val_acc: 0.7040\n",
      "Epoch 43/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6635 - acc: 0.7823 - val_loss: 0.8280 - val_acc: 0.7107\n",
      "Epoch 44/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6525 - acc: 0.7826 - val_loss: 0.8739 - val_acc: 0.6934\n",
      "Epoch 45/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6526 - acc: 0.7848 - val_loss: 0.8455 - val_acc: 0.7030\n",
      "Epoch 46/50\n",
      "218/218 [==============================] - 41s 190ms/step - loss: 0.6532 - acc: 0.7843 - val_loss: 0.8485 - val_acc: 0.7135\n",
      "Epoch 47/50\n",
      "218/218 [==============================] - 42s 191ms/step - loss: 0.6550 - acc: 0.7863 - val_loss: 0.8128 - val_acc: 0.7174\n",
      "Epoch 48/50\n",
      "218/218 [==============================] - 41s 188ms/step - loss: 0.6416 - acc: 0.7893 - val_loss: 0.8765 - val_acc: 0.7053\n",
      "Epoch 49/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6417 - acc: 0.7923 - val_loss: 0.8427 - val_acc: 0.7058\n",
      "Epoch 50/50\n",
      "Epoch 49/50\n",
      "218/218 [==============================] - 41s 189ms/step - loss: 0.6329 - acc: 0.7972 - val_loss: 0.8303 - val_acc: 0.7224\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {'dim': (256,256,1),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 4,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "\n",
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=training_generator, \n",
    "                              validation_data=validation_generator, \n",
    "                              use_multiprocessing=True, \n",
    "                              workers=6,\n",
    "                              epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
